---
layout: post
title:  "천재교육 빅데이터 7기 강의 정리 -5"
excerpt: "천재교육 5"

tags:
  - [Blog, jekyll, Github, Python, 천재교육, 빅데이터, 데이터EDA, 데이터 EDA, 데이터분석, 데이터 분석]

toc: true
toc_sticky: true
 
date: 2024-02-05
last_modified_at: 2024-02-05
---

{% include toc.html %}

### 2월 5일 강의

저번주에 실습 프로젝트에 이어서 오늘은 Github사용법, 그리고 판다스 더 잘 다루는 방법을 배웠다.   
이전 강좌와 비슷한 유형이 많았지만 살짝 더 깊게 들어가서 새롭게 배우는 것도 있었다.   




#### 데이터 EDA
탐색적 데이터 분석 (Exploratory Data Analysis)   
데이터를 분석하기 위해서 데이터의 형태, 성질 등을 파악하기 위해서 진행한다.

#### Data 기술(describe)통계란?
기술통계는 데이터 분석 및 설명 목적으로 수집된 데이터를 확률/통계(mean,median,sum 등)적으로 보는 것이다.   
초기 단계에서 데이터의 분포와 특징을 파악하기 위해서 산출한다.

#### 용어 간단 정리
1. data : 넓은 개념으로 정보를 의미함
2. Tidy data : 데이터 분석하기 깔끔한 데이터(전처리 할 필요가 많이 없는것)
3. 수치형(Numerical) : 정수,실수 등 숫자로 이뤄진 연속형 데이터
4. 범주형(Categorical) : 수치화 할 수 없는 데이터, 일반적으로 문자로 이뤄져있고 숫자의 경우 범주형일 경우 범주형으로 봐야함
5. 명목형(Nominal) : 순서가 없는 범주형 데이터(서울, 부산, 대구 등)
6. 서수형(Ordinal) : 상대적 위치 혹은 순서가 있는 범주형 데이터(1번째줄, 2번째줄, 3번째줄)
7. 행(row) / 레코드(record) / 필드(field) : 데이터 프레임의 수평으로 배열된 한줄을 의미함
8. 열(column) : 수직으로 한줄. 변수 또는 특성이라고 함
9. 인덱스(index) : 행을 식별하는데 사용되는 것
10. 셀(cell) : 행과 열이 교차하는 지점
11. 값(value) : 한개의 데이터 포인트
12. N/A, NaN, Null : 결측값, 값이 없음을 나타냄

#### 데이터 불러오기와 이상한 데이터 처리 방법들
##### sep을 사용해서 데이터를 나눠서 불러오기
```python
import pandas as pd
data = pd.read_csv('../data.csv',sep=';') #어떤 데이터가 ;을 기준으로 나눠져있어서 그냥 부르면 한 열에 모든 데이터가 담겨버린다면 sep (구분자)로 나누는 포인트를 나눠줄 수 있다. 
```
데이터의 모양과 처리를 확인하고 싶다면 [깃허브](https://github.com/gustjr7879/chunjae) 링크에 **jae7.ipynb**를 확인하면 된다.   
##### 헤더 관련 에러
한 데이터를 불러올 때 헤더가 데이터로 설정되는 경우가 있다. 즉 헤더가 없어서 첫번째 행을 헤더로 인식하는 경우인데 이런 경우에는 직접 헤더를 설정해주는 방법이 있다.
```python
flag_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/flags/flag.data'
flag_df = pd.read_csv(flag_data) #그냥 불러오게 되면 1. 아프가니스탄이 header로 지정되는 경우가 있다.

print(flag_df.head(2)) 
#이런 경우엔 직접 columns를 지정해주는 방법이 있다. 

column_headers = ['name', 'landmass', 'zone', 'area', 'population',
                  'language', 'religion', 'bars', 'stripes', 'colours',
                 'red', 'green', 'blue', 'gold', 'white',
                  'black', 'orange', 'mainhue', 'circles', 'crosses',
                  'saltires', 'quarters', 'sunstars', 'crescent', 'triangle',
                  'icon', 'animate', 'text', 'topleft', 'botright']
flag_df.columns = column_headers
print(flag_df.head(2))
```
위의 코드와 같이 처리하면 헤더가 잘 설정되는 것을 알 수 있다. 다만 이 방법은 헤더가 뭔지 전부 알고 있을 때 사용 가능한 방법이다.

##### 인덱스 문제
어떤 데이터의 인덱스가 중복되는 경우가 있다. 그런 경우에는 인덱스로 지정해주고, 중복되는 열을 drop해주면 깔끔해진다.
```python 
unnamed_df = pd.read_csv('../users.csv')
print(unnamed_df.head(2))
unnamed_df.index = unnamed_df['Unnamed: 0'] #인덱스로 지정 후
unnamed_df = unnamed_df.drop('Unnamed: 0',axis=1) #기존에 있던 column drop해서
print(unnamed_df.head(2)) #unnamed가 인덱스가 된 데이터 프레임 생성
```

#### 데이터 전처리 실습
1. 결측치 확인 및 처리
2. 중복값 확인 및 처리
3. 데이터 타입 변경 및 처리   
이렇게 세가지를 중점으로 배웠는데 여기에서 중복값을 처리하는 것은 처음 배웠다.   
그리고 데이터 전처리하는 것은 항상 기본중에 기본이니까 익숙해지도록 하자.
```python
#결측치 확인
academic = pd.read_csv('../academic.csv')
print(academic.head(3))
print(academic.isnull(),academic.isna()) #결측치 T/F로 표현
print(academic.isna().sum()) #어떤 컬럼에서 몇개 있는지 확인
academic2 = academic.fillna(0) #NaN값 0으로 채워놓기
print(academic2.head(3))
academic3 = academic.dropna(how='any') #하나라도 nan이 있으면 행 제거해버리기.
print(academic3.head(3))
academic4 = academic.dropna(how='any',axis=1) #하나라도 nan이 있으면 열 제거
print(academic4.head(3))
```
위의 코드는 결측치를 처리하는 것이다. isnull, isna 둘다 사용해도 되며, sum을 통해서 결측치를 무시해도 될 정도인지, 채우는게 나을지 판단할 수 있다.   
또한 dropna를 할 때 how를 통해서 하나라도 nan값이 있으면 제거할 수 있고, all로 설정하면 전체가 nan일때만 제거한다.   

#### 컨디셔닝
데이터 프레임에서 중복된 값만 본다던지, 특정 조건을 갖추는 것만 확인하고 조작하는 것을 컨디셔닝이라고 한다.   
##### 중복값 확인
중복값을 확인하는 방법은 df.duplicated()로 확인할 수 있다. keep 매개변수를 사용해서 맨 처음을 남길건지 맨 뒤를 남길건지 선택할 수 있다.   
##### 모양 확인
shape는 데이터의 모양을 보여준다. 양적인 부분을 확인할 수 있다.   
##### 데이터 타입 변경처리
날짜 데이터 처리(to_datetime)   
int, float 타입 변경   
범주형 -> 수치형으로 변환   
string 처리
```python 
user_df = pd.read_csv('../users2.csv')
print(user_df.head(3))
user_df = user_df.drop('Unnamed: 0',axis=1)
print(user_df.head(3))
print(user_df.dtypes) #행이 어떤 특성을 가지고 있는지 보여준다. date 가 object로 인식되고 있음을 알 수 있다.
```
위와 같이 dtypes을 사용해서 각 컬럼의 데이터 타입을 확인할 수 있다. 
```python
user_df['date'] = pd.to_datetime(user_df['date'])
print(user_df.dtypes) #datetime 타입으로 변환된 것을 확인할 수 있다. 
#datetime으로 바꾸면 날짜간의 차이를 계산할 수 있다. 
user_df['date_diff'] = user_df['date'].diff().dt.days #diff를 사용해서 앞에 행과의 날짜 차이를 확인할 수 있다.
print(user_df.head(5))
```
pd.to_datetime을 사용해서 datetime으로 바꿀 수 있고, datetime은 diff().dt.days를 사용해서 차이또한 계산할 수 있는 장점이 있다.   
```python
#또한 datetime으로 바꿈으로서 year, month 등의 정보를 쉽게 가져올 수 있다.
user_df['years'] = user_df['date'].dt.year
user_df['month'] = user_df['date'].dt.month
print(user_df.head())
print(user_df.dtypes)
#결측치 0으로 
user_df['date_diff'] = user_df['date_diff'].fillna(0)
print(user_df.isna().sum()) #결측치 0개인거 확인
```
또한 datetime으로 바꿔놨으니 dt.year, dt.month 등을 통해서 년 월을 쉽게 가져올 수 있고, 새로운 컬럼으로 지정할 수 있다.    
타입의 변환은 astype을 사용할 수 있다.   
```python
#date diff 타입을 int로 변환해보자 
user_df['date_diff'] = user_df['date_diff'].astype(int) #astype을 사용해서 변경할 수 있다. apply를 사용해도 되긴함
print(user_df.dtypes) #int로 바뀐 것 확인
```
이처럼 astype으로 해당 컬럼의 자료형을 변환할 수 있고, apply도 가능하지만 코드를 간결하게 짜기 위해서는 astype이 더 좋아보인다.   
str의 처리는 다음과 같이 사용할 수 있다.
```python
academic['split_year'] = academic['year'].str.split('/',expand=True)[0] #나누고 앞에꺼
academic['iso_week'] = academic['year'].str.split('/',expand=True)[1] #나누고 뒤에꺼
print(academic.head()) #split을 통해서 앞 부분과 뒷 부분이 나뉜것을 볼 수 있다.
```

#### Groupby 실습
그룹바이는 하나의 그룹으로 묶는다는 코드이다. 년도별 ~구하기 성별별 ~구하기 등 범주형으로 이루어진 애들을 그룹화 시켜서 한눈에 볼 수 있다.   
```python
# id를 떨구고, Gender로 그룹화하는데 min(최솟값)을 기준으로 해서 성별별 최소 값을 추출할 수 있다. 
insurance_df.drop(columns=['id']).groupby(['Gender']).min()
insurance_df.drop(columns=['id']).groupby(['Gender']).max()
insurance_df.drop(columns=['id', 'Vehicle_Age', 'Vehicle_Damage']).groupby(['Gender']).mean()
insurance_df.drop(columns=['id', 'Vehicle_Age', 'Vehicle_Damage']).groupby(['Gender']).count()
insurance_df.drop(columns=['id', 'Vehicle_Age', 'Vehicle_Damage']).groupby(['Gender']).nunique()
insurance_df.drop(columns=['id', 'Vehicle_Age', 'Vehicle_Damage']).groupby(['Gender']).median()
insurance_df.drop(columns=['id', 'Vehicle_Age', 'Vehicle_Damage']).groupby(['Gender']).agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None)
```
맨 위부터 최소값, 최대값, 평균, 개수, 중복없는 개수, 중간값, 최빈값이다.   
피벗테이블은 자주 안쓰여서 블로그에선 패스하도록 한다.   
원한다면 깃허브 들어가서 확인해볼 수 있다.

#### concat과 merge
1. concat : 동일한 컬럼 형태를 가진 데이터프레임의 row를 합치는 것(수직)
2. merge : 수평으로 붙임
```python
insurance_df_female = insurance_df[insurance_df['Gender']=='Female']
print(insurance_df_female.shape)
insurance_df_male = insurance_df[insurance_df['Gender']=='Male']
print(insurance_df_male.shape)

insurance_df_concat = pd.concat([insurance_df_female,insurance_df_male])
print(insurance_df_concat.shape)

insurance_df_mean = insurance_df.drop(columns=['id','Vehicle_Age','Vehicle_Damage']).groupby(['Gender']).mean().reset_index()[['Gender', 'Annual_Premium']].rename(columns={'Annual_Premium': 'Annual_Premium_mean'})
insurance_df_sum = insurance_df.drop(columns=['id','Vehicle_Age','Vehicle_Damage']).groupby(['Gender']).sum().reset_index()[['Gender', 'Annual_Premium']].rename(columns={'Annual_Premium': 'Annual_Premium_sum'})
#reset index는 groupby를 하게되면 gender가 index가 되는데, reset_index를 해서 젠터를 컬럼으로 사용하고, rename을 사용해서 컬럼의 이름을 바꿔준다. 

insurance_df_merge2 = insurance_df_mean.merge(insurance_df_sum,how='left',on='Gender')
print(insurance_df_merge2)
insurance_df_merge = pd.merge(insurance_df_mean,insurance_df_sum,how='left',on='Gender')
print(insurance_df_merge)
```
위는 concat과 merge의 예시이다.

#### 데이터 EDA 실습
크게 3개로 나눈다.   
1. 데이터 기술통계 확인
2. 데이터 분포 확인(시각화)
3. Pandas profiling   
```python
print(insurance_df.shape)
print(insurance_df.dtypes)
print(insurance_df.index)
print(insurance_df.columns) #컬럼 뭐가 있는지 확인
print(insurance_df['Response'].value_counts(normalize=True)) # 값의 비중을 확인할 수 있다.
print(insurance_df['Response'].value_counts(normalize=False)) # 비중이 아닌 직접적인 수치를 확인할 수 있다. 
```
value_counts로 갯수를 셀 수 있고 norm을 통해서 비중을 볼 수 있다.
```python
print(insurance_df.describe(include='all')) #groupby 하지 않고 빨리 확인할 수 있다. 각각의 컬럼에 있는 것들 모두 확인 가능
print(insurance_df.describe()) #top : 최빈값 , freq : 최빈값의 갯수
```
describe를 통해서 각각의 컬럼이 어떤 통계적 성질을 가지고 있는지 바로 뽑아볼 수 있다.
```python
#최종정리
print('평균: ', insurance_df['Age'].mean()) # 평균
print('중앙값: ', insurance_df['Age'].median()) # 중앙값 = df['키'].quantile(0.5)
print('최빈값: ', insurance_df['Age'].mode())  # 최빈값
print('표준편차: ', insurance_df['Age'].std()) # 표준 편차
print('표본분산: ', insurance_df['Age'].var()) # 표본 분산
print('합계: ', insurance_df['Age'].sum()) # 합계
print('최소값: ', insurance_df['Age'].min()) # 최소값
print('최대값: ', insurance_df['Age'].max()) # 최대값
print('범위:', insurance_df['Age'].max()-insurance_df['Age'].min()) # 범위
print('누적합: ', insurance_df['Annual_Premium'].cumsum().tail(1))
print('고유한 원소 개수: ', insurance_df['Age'].nunique())
print('1분위수: ', insurance_df['Age'].quantile(0.25))
print('3분위수: ', insurance_df['Age'].quantile(0.75))
print('첨도: ', insurance_df['Age'].kurtosis())
print('왜도: ', insurance_df['Age'].skew())
print('공분산: ', insurance_df['Age'].cov(insurance_df['Annual_Premium']))
print('상관계수: ', insurance_df['Age'].corr(insurance_df['Annual_Premium']))
```
위와 같이 하나하나 뽑아서 볼 수 있다.
```python
print(insurance_df.drop(columns=['id', 'Vehicle_Age', 'Vehicle_Damage', 'Gender']).corr())
```
를 통해서 각 변수들의 상관계수(회귀/인과관계 아님)를 분석할 수 있다. 상관계수(corr)의 해석은 다음과 같다.    
1. r이 -1.0과 -0.7 사이이면, 강한 음적 선형관계
2. r이 -0.7과 -0.3 사이이면, 뚜렷한 음적 선형관계
3. r이 -0.3과 -0.1 사이이면, 약한 음적 선형관계
4. r이 -0.1과 +0.1 사이이면, 거의 무시될 수 있는 선형관계
5. r이 +0.1과 +0.3 사이이면, 약한 양적 선형관계
6. r이 +0.3과 +0.7 사이이면, 뚜렷한 양적 선형관계
7. r이 +0.7과 +1.0 사이이면, 강한 양적 선형관계    
#### 판다스 프로파일링
판다스 프로파일링은 좀 오래걸리지만 좋다. 필요하다면 깃허브에 들어가서 html파일로 어떤식으로 처리되었는지 확인하도록 하자.


#### 후기
오늘은 약간 복습의 느낌이 조금 더 강했다. 하지만 새롭게 배우는 데이터 분석 방법들과 반복되어서 실습한다는 것은 실무에서 그만큼 비중있게 사용되는 기본중의 기본 스킬이라고 생각하고, 많은 데이터셋을 통한 실습으로 좀 더 본격적으로 코드를 짠 것 같다.   
또한 판다스 프로파일링이라는 개꿀 분석 방법을 배웠고 마지막으로 과제를 하면서 한번 더 머릿속에 집어넣음으로서 나만 복습을 한다면 까먹을리는 없을 것 같다.   
또한 반복해서 코드를 만듦으로서 검색했을 때 빠르게 찾고 오류를 해결할 수 있는 능력이 생겼다.   
코드에 따라서 데이터의 모양이 어떻게 변화하는지 확인하고 싶다면 깃허브에 들어가서 확인하도록 하자.