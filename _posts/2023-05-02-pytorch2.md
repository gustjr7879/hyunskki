---
layout: post
title:  "Deep learning-2"
excerpt: "Deep Learning-2"

tags:
  - [Blog, jekyll, Github, Python]

toc: true
toc_sticky: true
 
date: 2023-05-01
last_modified_at: 2023-05-01
---
{% include toc.html %}


### 1. 자동 미분 (Autograd)
경사하강법에 대한 코드를 보고 있으면 requires_grad = True와 같은 코드를 본적이 있을 것이다. 이는 파이토치에서 제공하는 자동미분 기능을 사용하고 있다는 뜻이다.

#### 1) 경사하강법 리뷰
경사하강법을 다시 복습해보자. 경사하강법은 loss function을 미분하여 이 함수의 기울기를 구해서 비용이 최소화되는 방향을 찾아내는 알고리즘이다.
다시 말해서 loss가 최소화 되는 방향을 찾아나가는 것이 경사하강법이다.
모델이 복잡할 수록 경사하강법을 직접 코딩하기에는 어려움이 존재하니, 파이토치에서는 기능을 제공하고 있다.

#### 2) 자동미분(Autograd) 실습
{% highlight css %} 
import torch
w = torch.tensor(2.0,requires_grad = True)
y = w**2
z = 2*y + 5 # z = 2*w**2 + 5
z.backward()
print(w.grad) # 수식을 w로 미분한 값
{% endhighlight %}
위의 수식을 실행했을 때 8.0이 출력되는 것을 볼 수 있다.

### 2. 다중 선형 회귀 (Multivariable Linear regression)
앞에서 배운 x가 1개인 선형회귀를 단순 선형회귀라고 하고, 다수의 x로 부터 y를 예측하는 것을 다중 선형 회귀라고 한다.
다른점은 data x의 개수가 1개가 아닌 여러개라는 차이가 존재한다.

H(x) = w1x1 + w2x2 + w3x3 + b
와 같이 표현할 수 있다.
변수의 개수마다 weight가 붙는다 이는 어떤 것을 의미하는지 알아보도록 하자

파이토치로 실습해보자.
{% highlight css %} 
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
torch.manual_seed(1)
#훈련 데이터 선언
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
#가중치 w와 편향 b 초기화
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

#optimizer 설정
optimizer = optim.SGD([w1,w2,w3,b],lr = 1e-5)
epochs = 1000
for i in range(epochs):
    hypo = x1_train*w1 + x2_train*w2 + x3_train*w3 + b
    loss = torch.mean((hypo - y_train)**2)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if i %100 == 0:
        print(loss)
print(w1.item(),w2.item(),w3.item(),b.item())
{% endhighlight %}
하게되면 최종적인 loss 는 1.1224가 나오게 되고, 이때 w1,w2,w3,b를 알 수 있다.

확실히 실습을 따라해보니까 좀 더 명확하게 어떤것을 사용하는지 알 수 있게 되는 것 같다.
좋은 공부방향인듯 !

#### 1) 벡터와 행렬 연산으로 바꾸기
방금했던 실습 코드를 개선할 수 있는 방법이있다.
현재는 x의 개수에 따라서 계속 w의 개수또한 증가되는 방법을 사용하고 있는데, 만약에 x의 개수가 1000개가 된다면 상당히 귀찮아질 수 있다. 
이를 해결하기 위하여 행렬 곱셈 연산(또는 벡터의 내적)을 사용한다.

행렬의 곱셈과정에서 이루어지는 벡터연산을 벡터의 내적이라고 한다.(Dot product) 이 벡터의 내적을 왜 말하는 걸까?

#### 2) 벡터 연산으로 이해하기
H(x) = W1x1 +W2x2 + W3x3 + b 식은
1x3인 x 벡터와, 3x1인 w벡터의 내적으로 표현할 수 있다. 따라서 x의 개수가 3개였음에도 x벡터와 w벡터 2개의 벡터로 표현되는 것을 확인할 수 있다.
벡터의 행렬연산은 식을 간단하게 해줄뿐만 아니라 병렬연산으로 사용되어서 속도에서 이점을 가질 수 있다.

위의 실험을 벡터연산으로 처리하여서 코드를 작성하면,
{% highlight css %} 
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
torch.manual_seed(1)

x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

# 모델 초기화
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=1e-5)
epochs = 1000
for i in range(epochs):
    hypo = x_train.matmul(W) + b #b는 브로드ㅜ캐스팅되어서 더해진다.
    loss = torch.mean((hypo - y_train)**2)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if i %100 == 0:
        print(loss)
print(W,b.item())
{% endhighlight %}

위와 같이 표현할 수 있다.

### 3. nn.Module로 구현하는 선형회귀
방금 실습까지 우리는 직접 가설, loss function을 직접 정의해서 선형회귀 모델을 구현하였다. 하지만 nn.Module 에 이미 구현되어있는 것들을 토대로 불러와서 더 쉽게 선형회귀 모델을 만들어보자.

예를들어서 파이토치에는 선형회귀모델이 nn.Linear()로 , MSE loss function이 nn.functional.mse_loss() 라는 함수로 만들어져있고 이를 그냥 불러오기만 하면 된다. 

{% highlight css %} 
import torch.nn as nn
model = nn.Linear(input_dim, output_dim)
import torch.nn.functional as F
loss = F.mse_loss(prediction, y_train)
{% endhighlight %}
와 같이 사용할 수 있다. 이를 사용하여 전체적으로 실습해보도록 하자

{% highlight css %} 
import torch
import torch.nn as nn
import torch.nn.functional as F
torch.manual_seed(1)
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
model = nn.Linear(1,1)
#모델에는 가중치와 편향이 저장되어있는데, 이는 model.parameter()라는 함수를 사용하여서 불러올 수 있다.
print(list(model.parameter()))
optimizer = torch.optim(model.parameter(),lr = 1e-2)

epochs = 2000
for i in range(epochs):
    pred = model(x_train)
    loss = F.mse_loss(pred,y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(loss.itme())
{% endhighlight %}

실습파일에는 다중선형회귀 코드도 작성해서 올려놨다. 필요할 때 확인하면 될 것 이다.
그리고 이제 실습할때 그냥 다중 선형회귀를 기본으로 하여서 진행할 예정이다.

### 4. 클래스로 파이토치 모델 구현하기
대부분의 모델들은 파이토치 모델을 클래스로 구현하고 있다. 앞에서 실습할 때 만든 코드들과 다른 점은 클래스로 구성했다는 것이다.

{% highlight css %} 
class LinearRegressionModel(nn.Module): #torch.nn.Module을 상속받는 클래스라는 의미
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1,1)
    def forward(self,x):
        return self.linear(x)
model = LinearRegressionModel()

class MultivariateLinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.

    def forward(self, x):
        return self.linear(x)
model = MultivariateLinearRegressionModel()

{% endhighlight %}
위와 같이 클래스로 선언하고 사용할 수 있다.
이렇게 선언하는 것을 연습하는 것이 좋다. 대다수의 모델들은 클래스로 만들어져있기 때문에 사용 및 수정을 하려면 클래스로 만들어 버릇해서 사용하자

### 5. 미니배치와 데이터로드
기업이나 다른 프로젝트에서 사용하는 데이터셋의 크기는 매우 크다. 매우 큰 데이터셋 전체를 가지고 기울기를 계산하여서 경사하강법을 진행하기에는 상당히 무리가 존재한다.
따라서 전체 데이터를 작은 단위로 만들어서 해당 단위로 학습을 진행하는 개념이 나오게 되었는데, 이를 미니배치라고 한다.
미니배치 학습을 진행하면, 미니배치만큼 가져가서 미니배치에 대한 학습을 진행하고(loss계산) 경사하강법을 진행하고, 그 다음 미니배치를 가져가서 학습하는 식으로 전체 데이터에 대한 학습을 진행합니다.
이렇게 전체 데이터를 돌면 1epoch를 진행했다고 합니다.

이렇게 하면 값을 헤매기는 하는데 훈련속도가 빠르다는 장점이 있다.
보통 2의 제곱수를 사용한다.

그 외로 데이터 로더 형식을 어떻게 작성하느냐, 커스텀 데이터셋을 어떻게 생성하는지에 대한 이야기도 나오지만 이 내용은 나중에 필요할 때 보는 것이 좋을 것 같다.

https://wikidocs.net/57805

에 들어가면 확인할 수 있다.
