---
layout: post
title:  "천재교육 빅데이터 7기 강의 정리 -9"
excerpt: "천재교육 9"

tags:
  - [Blog, jekyll, Github, Python, 천재교육, 빅데이터, 데이터EDA, 데이터 EDA, 데이터분석, 데이터 분석, 크롤링]

toc: true
toc_sticky: true

date: 2024-02-15
last_modified_at: 2024-02-15
---

{% include toc.html %}

### 2월 15일 강의 정리
[깃허브 링크1](https://github.com/gustjr7879/chunjae/blob/main/jae11.ipynb)      
[깃허브 링크2](https://github.com/gustjr7879/chunjae/blob/main/hw5.ipynb)        

오늘은 **정적**인 페이지에서의 웹 크롤링을 학습 및 실습했다.     
크롤링은 예전부터 해보고 싶었고 나만의 크롤러 하나쯤은 만들어놓고 싶었지만 엄두도 못내다가 이번 기회에 해봤다.    
HTML을 우선적으로 이해했고 그다음에 정적인 페이지를 크롤링 하는 방법과 동적인 페이지를 크롤링 하는 방법에는 뭐가 있는지 알게 되었다.     
전혀 접해보지 못했던 것이라서 코드도 손에 잘 안붙고 헤매는 시간이 길었다.    
그래도 나름대로 기준을 찾아서 실습까지 진행할 수 있었던 시간이였다.   

#### 웹 크롤러와 웹 스크래퍼
웹 크롤링과 웹 스크래핑은 원래 구분되는 개념이지만, 한국 사회 실무에서는 구분 없이 사용한다.    
- 웹 크롤링 : 웹 크롤러가 다양한 페이지를 인덱싱한다.
- 웹 스크래핑 : 웹 사이트의 데이터를 추출해서 저장한다.    
웹 크롤링은 웹 구조에 따른 파싱(parsing) 작업이 이루어지므로 단순 복붙과는 다른 특성을 보인다.     
html을 다 뒤져서 복사,, 붙여넣기 한다고 생각하면 벌써부터 아찔하다.     
파이썬에서 사용하는 웹 크롤링 패키지는 Selenium(동적 크롤링 : 사용자와의 상호작용을 포함하고 있는 웹) / Beautiful Soap(정적 크롤링)으로 구분된다.     

```python
#뉴스페이퍼 패키지를 사용해서 뉴스를 크롤링해보자
from newspaper import Article
link = 'https://www.yonhapnewstv.co.kr/news/MYH20240214022300641'
article = Article(link,language='ko')
article.download()
article.parse()
title = article.title
text = article.text
date = article.publish_date
print(title)
print(text[57:])
print(date)
```

위와 같이 뉴스페이퍼 패키지를 사용하면 url과 단순 작업을 통해서 크롤링할 수 있지만, 세밀하게 설정하지 못한다는 점이 단점이다. 하지만 빠르고 쉽게 제목과 내용을 파악하기엔 편리한 방법이다.     
다음은 정적 크롤링의 BeautifulSoap이다.    

```python
from bs4 import BeautifulSoup
import requests
import pandas as pd

#알라딘 url
page = 200000 #순위별로 페이지 다름
url = f'https://aladin.co.kr/shop/common/wbest.aspx?BestType=Bestseller&BranchType=1&CID=0&page={page}&cnt=1000&SortOrder=1'

req = requests.get(url) #html파일 요청해주는 역할 : request
content = req.content
print(content) #content = html 파일 내용
soup = BeautifulSoup(content,'html.parser')
#html 파일 분석 결과 a.bo3 클래스의 b태그에 책 이름이 들어있다.
#또한 span.ss_p2 의 span에 가격이 들어있다.
book_names = soup.select('a.bo3 b') #a.bo3클래스 b태그
prices = soup.select('span.ss_p2 span') #가격
print(book_names[0].text)
print(int(prices[0].text.replace(',','')))
# 크롤링해서 제목과 가격만 dataframe으로 저장
df = pd.DataFrame(columns=['책제목','가격'])
all_name = []
all_price = []
for i in range(1,4):
    page = i
    url = f'https://aladin.co.kr/shop/common/wbest.aspx?BestType=Bestseller&BranchType=1&CID=0&page={page}&cnt=1000&SortOrder=1'
    req = requests.get(url)
    content = req.content
    soup = BeautifulSoup(content,'html.parser')
    book_names = soup.select('a.bo3 b') #a.bo3클래스 b태그
    prices = soup.select('span.ss_p2 span') #가격
    length = len(book_names)
    for j in range(length):

        all_name.append(book_names[j].text)
        all_price.append(int(prices[j].text.replace(',','')))
leng = len(all_name)
df['책제목'] = [all_name[j] for j in range(leng)]
df['가격'] = [all_price[j] for j in range(leng)]

print(df)
df.to_csv('./알라딘크롤링결과.csv')
```

위와 같이 HTML의 클래스에 해당하는 것을 쉽게 긁어올 수 있고 반복문, 재귀함수 등을 통해서 계속해서 긁어올 수 있다.    
하지만 정적페이지에서만 작동하고 동적페이지에서 하려고 한다면 많은 제약이 따른다.    
실습을 진행하려고 다양한 페이지에서 크롤링을 해보려고 했지만 많은 사이트에서 크롤러에게 정보를 제공해주지 않았고 동적페이지가 많아서 beautifulsoap를 가지고는 깊게 진행하지 못했다.     

#### 마치며
오늘은 거의 실습위주로 페이지를 탐색하고 구조를 확인하고 크롤러를 사용해서 크롤링하는 위주로 수업이 진행되었다.    
실제로 동작한다는 것이 신기했고 내일 동적 크롤링을 배워가지고 얼른 다양한 사이트에 적용하고 익숙해져서 나만의 크롤러를 하나 만들어야겠다.    
실습은 내 블로그 홈페이지로 진행했고 코드를 확인하고 결과를 보고 싶다면 맨 위에 링크에 접속해서 확인할 수 있다.     
