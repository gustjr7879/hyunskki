---
layout: post
title:  "천재교육 빅데이터 7기 강의 정리 -7"
excerpt: "천재교육 7"

tags:
  - [Blog, jekyll, Github, Python, 천재교육, 빅데이터, 데이터EDA, 데이터 EDA, 데이터분석, 데이터 분석, 상관계수, 회귀계수, 회귀분석, 통계]

toc: true
toc_sticky: true
 
date: 2024-02-07
last_modified_at: 2024-02-07
---

{% include toc.html %}

### 2월 7일 강의 정리
[깃허브 링크1](https://github.com/gustjr7879/chunjae/blob/main/jae9.ipynb)      
[깃허브 링크2](https://github.com/gustjr7879/chunjae/blob/main/hw3.ipynb)      
오늘은 상관분석, 선형 회귀분석, 로지스틱 회귀분석까지 배웠다.    
항상 어렴풋이 알고는 있었던 내용이지만, 왜 쓰는지 어떤 상황에서 쓰는지 넘어갔기 때문에 이번에 좀 머리를 쓰면서 들었다.     
저번에는 가설을 세우고 가설 검정을 했었는데, 이때 주로 사용한 방법들은 **변수들 간의 차이 확인**을 주 목표로 하는 방법들이다.     
오늘은 **변수들 간의 관계**를 설명하기 위한 방법들이다.     

#### 상관분석 (Correlation Analysis)
1. 공분산 (Covariance)     
- 두 변수 X와 Y가 함께 변하는 정도를 보여준다.     
공분산이 양수인 경우에는 두 변수가 함께 증가한다는 것을 보여주고, 음수인 경우엔 함께 감소한다는 것을 보여준다. 공분산의 절대값이 클수록 두 변수간의 관계가 강하다는 뜻이다.    
단, 두 변수의 단위가 다르면 적용하기 어려우므로 표준편차로 나누어서 정규화한다.    

2. 상관계수 (Correlation Coefficient)
- 피어슨 상관계수(Pearson) : 두 변수간의 **선형적인 관계**를 측정한다. 두 변수는 무조건 **연속형**이여야한다.    
- 스피어만 상관계수(Spearman's) : 두 변수간 **의존성**을 측정한다. **범주형**데이터에서도 특히 **순서**가 있는 변수 간 상관관계를 측정한다.
- 일반 범주형(nominal) 변수에서의 상관관계는 카이제곱검정을 사용한다.
- 상관관계는 말 그대로 상관이 있다. 이지 영향을 끼친다 는 아니고 인과관계를 설명하는 것도 아니다.    
     
공분산과 피어슨 상관계수, 그리고 스피어만 상관계수를 구하는 방법은 다음과 같이 구할 수 있다.    

```python
covar = bank['age'].cov(bank['balance']) #나이와 통장잔고에 대한 공분산
print('공분산 : ',covar) 

from scipy.stats import pearsonr
pearson_corr, p_val = pearsonr(bank['age'],bank['balance'])
print('피어슨 상관계수',pearson_corr)
print('p_val',p_val) #p-value

from scipy.stats import spearmanr
import numpy as np
import pandas as pd
X = [1, 2, 3, 4, 5]
Y = [5, 4, 3, 2, 1]

#스피어만 상관계수 
rho,p_val = spearmanr(X,Y)

print('스피어만 상관계수',rho)
print('p_value',p_val)
```      

위와 같이 확인하게 된다면 p-value는 낮아서 통계적으로 연관성이 있다고 볼 수 있지만, 피어슨 상관계수는 낮아서 영향도가 크지 않다 라고 알 수 있다.    
스피어만 상관계수는 매우 크고 음의 상관성을 띄는 것을 확인할 수 있다.    
더 간단하게 다음과 같이 볼 수 있다.     

```python
bank.select_dtypes(exclude='object').corr() #데이터타입 object제외하고 선택
```    

위 코드와 같이 작성하면 바로 보여준다. 또한 corr안에 매개변수로 다른 상관계수 또한 확인할 수 있다.   
.select_dtypes를 잘 알아두자. 한번에 데이터프레임을 처리하기 편하다.    

#### 선형회귀 (Linear Regression)   
1. 변수   
- 독립변수(independent variable, x)를 부르는 다양한 이름 (Predictor variable, Features)   
- 종속변수(dependent variable, y)를 부르는 다양한 이름 (Label, Target)   

2. 회귀계수 (Regression Coefficient)
- 주어진 데이터로 선형회귀선을 그렸을 때 도출되는 계수
- 상관계수는 두 변수간 선형관계의 **강도와 방향**을 측정하는데 사용 가능. 회귀계수는 한 변수가 다른 변수에 미치는 **영향의 크기**를 나타낸다. 이 두가지 모두 인과관계를 나타내지 않음. **영향력**을 판단함.    

3. 선형회귀 (Linear Regression)
- 선형회귀선 : residual square들의 합인 RSS를 최소화하는 직선    
- 예측값 : 만들어진 모델(회귀선)이 추정하는 값    
- 잔차(Residual): 관측값과 예측값의 차이    
- 선형회귀는 데이터가 연속적인 데이터일 때 사용한다. 이산적인 데이터일 때는 로지스틱 회귀 분석을 진행한다.    

4. 선형회귀 모델 평가지표    
- 결정계수 $R^2$    
- + 종속 변수의 총 변동량 중 모델에 의해 설명되는 비율. 즉 결정계수가 0.65이다. 라고 한다면 이 모델은 데이터를 65% 설명할 수 있음을 나타낸다.    
- + MAE, RMSE 는 loss 함수이다.    

방갯수(Rooms)와 집값의 연관성을 보는 코드는 다음과 같다.    

```python
import numpy as np
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error

X = mel_df['Rooms']
y = mel_df['Price']

#X에 const를 더해준다 (상수)
X = sm.add_constant(X)

#모델 생성 
model = sm.OLS(y,X).fit() #데이터로 학습
pred = model(X) #모델이 추정하는 값

# R square 값
r_square = model.rsquared

#MAE
mae = mean_absolute_error(y,pred)

#모델 요약 
print(model.summary())
```

이렇게 단순선형회귀분석을 할 수 있고 두 변수간에 영향도가 어떤지 확인할 수 있다.   
다음은 다중선형회귀분석이다.    

```python
from sklearn.preprocessing import StandardScaler
#독립변수를 object가 아니고 Price를 제외한 나머지 사용
X = mel_df.select_dtypes(exclude='object').drop(columns=['Price'])
y = mel_df['Price']

#nan값을 제거하고 y의 index를 맞출 수 있는 코드는 다음과 같다. 
#X = X.replace([np.inf, -np.inf], np.nan).dropna()  # Remove infinite values
#y = y[X.index]  # Adjust y to match the indices of X after removing rows with missing values
#하지만 난 위에 EDA를 진행하면서 nan값을 처리해줬으므로 다음에 사용해보도록 한다.

#다양한 변수들의 scale을 맞춰주기 위해서 StandarScaler를 사용한다.
original_column_names = list(X.columns)
original_column_names.insert(0,'const')

scaler = StandardScaler()
X_scale = scaler.fit_transform(X) #scale맞춰준다.
X_scale = sm.add_constant(X_scale)

model = sm.OLS(y,X_scale).fit()

r_squared = model.rsquared

pred = model.predict(X_scale)

mae = mean_absolute_error(y,pred)
rmse = np.sqrt(mean_squared_error(y,pred))

print(model.summary(xname = original_column_names))
print(r_squared)
print(mae)
print(rmse)
```   

#### 로지스틱 회귀 (Logistic Regression)
선형회귀를 설명하면서 적어놓긴 했는데, 선형회귀분석은 target(종속변수)이 연속적인 데이터일 때 사용한다.   
반면에 지금 배워볼 로지스틱 회귀는 타겟이 이산적일 때 사용하는 회귀분석 방법이다.   
결과적으로 관측치가 특정 클래스에 속할 확률 값으로 계산된다.   
다음과 같이 적용해볼 수 있다.     
    
```python
X = insur.select_dtypes(exclude = 'object').drop(['Response'],axis=1)
y = insur['Response']

original_column_names = list(X.columns)
original_column_names.insert(0,'const')

scaler = StandardScaler()
X_scale = scaler.fit_transform(X)
X = sm.add_constant(X_scale)
logits = sm.Logit(y,X).fit()

summary = logits.summary(xname = original_column_names) #정규화하면 컬럼 이름이 날아가기 때문에 이렇게 해서 이름을 설정해줌.
print(summary)
print(logits.summary().tables[1])
```
위의 코드로 logist regression의 결과를 알아볼 수 있다.    
오늘 진행한 코드들의 결과는 모두 맨 처음 깃허브 링크를 들어가면 ipynb파일에서 확인할 수 있다.    
   
#### 학습 점검 체크리스트
- 공분산을 설명할 수 있는가 -> 두 변수 중 하나가 변할 때의 변화량
- 피어슨 상관계수와 스피어만 상관계수의 차이 -> 각각 연속형 / 범주형일 때 사용함
- 데이터의 선형성을 설명할 수 있는가 ? -> 각 데이터 포인트들로부터의 오차가 가장 적은 표현을 잘 하는 선   
- 회귀계수와 상관계수의 차이 -> 상관 : 선형관계의 강도 및 방향 / 회귀 : 변수 사이의 영향관계   
   

### 마무리
오늘은 직접 선형 회귀분석과 로지스틱 회귀분석을 해봤다.   
가장 중요한 부분은 변수들 사이의 **차이**가 아닌 **영향**을 확인하기 위해서 상관 분석과 회귀 분석을 진행한다는 것이다.    
지난 시간에 배운 통계적으로 p-value를 구하는 것은 변수들 사이의 **차이**를 보는 것이였다.    
또한 선형회귀분석은 종속변수가 연속형 데이터일때, 로지스틱회귀분석은 종속변수가 이산형 데이터일 때 사용한다는 것이다.    
오늘 배운 것에 대한 개념을 꼭 기억해두고 있도록 하자.