---
layout: post
title:  "Deep learning-1"
excerpt: "Deep Learning-1"

tags:
  - [Blog, jekyll, Github, Python]

toc: true
toc_sticky: true
 
date: 2023-05-01
last_modified_at: 2023-05-01
---
{% include toc.html %}


### 1. 머신러닝 용어 이해하기
이번에는 머신러닝의 특징들에 대해서 배우고, 용어를 어느정도 이해하고 넘어가는 시간을 가지자.

#### 머신러닝 모델의 평가
실제 모델을 평가하기 위해서 데이터를 훈련용 검증용 테스트용 이렇게 세가지로 분리하는 것이 일반적이다.
훈련용 데이터는 말 그대로 훈련할때 사용하는 데이터셋이라고 생각하면 되는데, 과연 검증용 데이터가 필요한 이유가 무엇일까? 검증용(validation dataset)은 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도이다. 더 정확하게는 과적합이 되고있는지 판단하거나, 하이퍼파라미터를 조정하기 위한 용도이다. 하이퍼파라미터란, 값에 따라서 모델의 성능에 영향을 주는 매개변수를 의미한다. 
훈련용 데이터로 학습시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝합니다.
만약에 데이터셋이 작아서 valid data와 test data를 나눌만큼이 아니라면 k-fold 교차검증이라는 또 다른 방법을 사용하기도 한다.

#### 분류(classification)와 회귀(regression)
분류는 보통 이진 분류 문제(binary classification)와 다중 클래스 분류(Multi class classification)으로 나뉘고 그 이외에 연속된 값을 결과로 가지는 것들은 회귀문제로 풀게된다.

#### 지도학습(supervised learning)과 비지도학습(unsupervised learning)
머신러닝은 크게 지도학습, 비지도학습, 강화학습으로 나눈다. 지도학습이란, 레이블과 함께 학습하는 것을 말한다. 이때 기계는 예측값과 실제값의 차이인 loss를 줄이는 방식으로 학습을 진행하게 된다. 
비지도학습이란, 레이블이 없는 학습방법으로, clustering이나 차원축소와 같은 학습방법이 있다.
강화학습은 보상을 최대화 하는 행동 혹은 행동 순서를 학습하는 방법이고 self supervised learning은 데이터 스스로 target이 되어서 패턴을 학습하는 모델이다.

이런식으로 나뉜다.
이외로 혼동행렬, 과적합 및 과소적합 등에 대한 내용이 나오고 이를 확인하고 싶으면 
https://wikidocs.net/60021 
에 들어가서 확인할 수 있다.

### 2. 퍼셉트론(perceptron)
인공신경망은 수많은 머신러닝 방법 중 하나이다. 하지만 최근 인공신경망을 복잡하게 쌓아 올린 딥러닝이 다른 머신러닝 방법들을 뛰어넘는 성능을 보여주고 있다. 딥러닝을 이해하기 위해서는 우선 인공신경망에 대한 이해가 필요하므로 초기의 인공신경망인 퍼셉트론을 공부하자.

#### 퍼셉트론(perceptron)
퍼셉트론은 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다. 
다수의 입력을 받아서 활성화 함수로 처리해줘가지고 일정 이상값이 되면 출력하게 해줍니다. 
값을 보내는 단계와 값을 받아서 출력하는 단계로 이루어져있는데, input layer, output layer라고 합니다.
단층 퍼셉트론의 한게를 극복하고자, 다층퍼셉트론 등을 내보였습니다. 
다층 퍼셉트론(Multilayer perceptron, MLP)이란 단층 퍼셉트론의 한계를 극복하고자 은닉층을 쌓아서(hidden layer) 한계를 없애는 방식이다.

이렇게 은닉층이 2개 이상인 신경망을 심층 신경망 (Deep Neural Network)라고 한다. 
앞에서 여러번 실습한 loss function과 역전파 등을 통하여 기계 스스로 최적의 해를 찾아내게 하는 것이다.

역전파에 대한 내용은 수학적인 내용이 많고, 복잡하기 때문에 꼭 
https://wikidocs.net/60682
에 들어가서 확인하자

#### 비선형 활성화 함수
앞에서 사용하던 활성화 함수는 시그모이드, 소프트맥스 같은 것들이 있었다. 하지만 모델이 학습하는 과정에서 시그모이드 함수는 사용을 지양해야한다. 그 이유를 한번 알아보자.

활성화함수의 특징은, 선형함수가 아닌 비선형 함수여야 한다는 점이다. 선형함수란 출력이 입력의 상수배만큼 변하는 함수를 선형함수라고 한다. 예를들어서 f(x) = Wx + b 라는 함수가 있을 때 W와 b는 상수이다. 이 식은 그래프로 나타내면 직선이 그려진다. 반대로 비선형 함수는 직선 1개로는 그릴 수 없는 함수를 나타낸다. 

인공신경망의 능력을 높기이 위해서는 은닉층을 계속해서 추가해야한다. 만약 활성화 함수로 선형함수를 사용하게 된다면 은닉층을 쌓을 수 없다. 왜냐하면 선형함수로 은닉층을 여러번 쌓게된다면 f(f(f(x))) 와 같은 형식이 되는데, 이는 W*W*W* 과 같이 처리가 되고 W의 세제곱을 k라고 하게된다면 y(x) = kx 처럼 1회만 처리된것처럼 된다. 그러기 때문에 차이가 존재하지 않게되므로 소용이 없다. 

#### 시그모이드 함수와 기울기 소실
시그모이드 함수를 활성화함수로 사용하는 어느 인공신경망이 있다고 가정해보자.
인공신경망은 입력에 대해서 순전파(forward propagation)연산을 하고 그리고 순전파를 통해 나온 예측값과 실제값의 오차를 loss function을 통하여 계산하고 미분을 통하여 기울기를 구하고 역전파를 진행한다. 
시그모이드 함수의 문제점은 미분을 해서 기울기를 구할 때 발생한다.
sigmoid 함수는 출력값이 0이나 1에 가까워지면 기울기가 완만해지는 것을 볼 수 있다. 하지만 역전파 과정에서 0에 가까운 작은 기울기가 곱해지면 앞쪽으로 가면 갈수록 기울기가 점점 작아져서 전달이 되지 않는다. 이를 기울기 소실(vanishing gradient)라고 한다. 
sigmoid 함수를 사용하는 은닉층의 개수가 많아질 경우에 발생하는 문제로 이러면 W가 업데이트 되지 않아 학습이 진행되지 않는 문제점이 발생한다.

따라서 다른 활성화함수가 연구되었다.

#### 하이퍼볼릭탄젠트 함수(Hyperbolic rangent function)
하이퍼볼릭탄젠트 함수(tanh)는 입력값을 -1과 1사이의 값으로 반환한다.
생긴것은 sigmoid와 매우 유사하게 생겼다. 그래서 -1이나 1에 가까운 경우 sigmoid 함수의 문제가 발생하는데 그러나 tanh함수는 시그모이드 함수와 다르게 0을 중심으로 하고 있다. 그래서 sigmoid와 비교하면 반환값의 변화폭이 조금 더 크므로 기울기 소실 증상이 적은 편이다.

#### 렐루 함수 (ReLU)
인공신경망에서 가장 많은 인기를 가지는 ReLU 함수이다. 수식은 f(x) = max(0,x)로 매우 간단하다.
렐루함수는 음수가 입력되면 0을 출력하고 양수를 입력하면 입력값을 그대로 반환한다. 렐루함수는 특정 양수값에 수렴하지 않으므로 깊은 신경망에서 시그모이드 함수보다 훨씬 더 잘 작동한다. 게다가 연산 속도 도 매우 빠른 편이다.
하지만 음수의 경우 기울기도 0이 되버리고, 0이 된 뉴런은 다시 회생하는 것이 매우 어렵다는 단점이 있다. 이를 (dying ReLU)

#### 리키 렐루 (Leaky ReLU)
죽은 렐루를 보완하기 위하여 ReLU의 변형함수들이 등장했다. 여러개가 있지만 가장 많이 사용되는 LeakyReLU에 대해서 알아보자.
LeakyReLU는 입력값이 음수일 경우에 0이 아니라 0.001과 같은 매우 작은 수를 반환하도록 하였다. 따라서 입력값이 음수라도 기울기가 0이 되지 않으면 ReLU가 죽지않아서 뉴런이 계속 입력을 받고 출력을 뱉을 수 있게 됩니다.

#### 소프트맥스 함수(softmax function)
hidden layer에서 ReLU 혹은 변형을 사용하는 것이 일반적이지만, 출력층에 softmax를 사용하여서 분류문제를 해결합니다.
출력증에 사용하여서 분류문제를 해결하는데, sigmoid의 경우는 이진 분류 문제에, softmax의 경우는 다중 클래스 분류 문제에 많이 사용된다.

#### 출력층의 활성화함수와 오차함수의 관계
이진분류의 출력층에는 sigmoid 활성화 함수를 주로 사용하고 loss는 BCELoss()를 주로 사용한다.
다중 클래스 분류의 출력층에는 sofmax 활성화 함수를 주로 사용하고 loss는 CrossEntropyLoss()를 주로 사용한다.


### 3. 다층퍼셉트론으로 손글씨 분류하기
이번에는 앞에서 공부한 다층 퍼셉트론을 구현하고 딥러닝을 통하여 숫자 필기 데이터를 분류해보자

#### 숫자 필기 데이터 소개
숫자필기데이터는 사이킷런 패키지에서 제공하는 분류용 예제 데이터이다. 0에서 9까지의 숫자를 손으로 쓴 이미지 데이터로 load_digits()명령으로 로드할 수 있다. 8*8 64 픽셀 해상도의 흑백 이미지이고, 1797개가 존재한다.

{% highlight css %} 
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

#첫번째 샘플 출력 .images[인덱스] 로 해당 인덱스의 이미지를 행렬로 출력가능

digits = load_digits()
print(digits.images[0])
#0을 흰색 도화지, 0보다 큰 숫자들을 검정색 점이라고 생각해보면 0같은 실루엣이 보임
#다음으로는 label을 출력해보자

print(digits.target[0])
#label 도 0이라고 나온다.
print(len(digits.images))
# 1797개의 이미지 샘플이 존재하는 것을 확인할 수 있다.

#몇개만 뽑아서 시각화해보자
images_and_labels = list(zip(digits.images, digits.target))
#for index, (image, label) in enumerate(images_and_labels[:5]): # 5개의 샘플만 출력
#    plt.subplot(2, 5, index + 1)
#    plt.axis('off')
#    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
#    plt.title('sample: %i' % label)
    #plt.show()

# 훈련 데이터와 레이블을 X,Y에 저장해보자. digits.data를 이용하여서 데이터를 얻을 수 있음

X = digits.data # image 의 feature
Y = digits.target # image의 label
# 다층 퍼셉트론 분류기 만들기
import torch
import torch.nn as nn
from torch import optim

model = nn.Sequential(
    nn.Linear(64,32),
    nn.ReLU(),
    nn.Linear(32,16),
    nn.ReLU(),
    nn.Linear(16,10) # label 이 10차원
)
X = torch.tensor(X,dtype=torch.float32)
Y = torch.tensor(Y,dtype=torch.int64)
loss_fn = nn.CrossEntropyLoss() # 이 loss함수는 소프트맥스 함수를 포함하고 있음
optimizer = optim.Adam(model.parameters())
losses = []

for epoch in range(100):
    optimizer.zero_grad()
    pred = model(X)
    loss = loss_fn(pred,Y)
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(loss)
    losses.append(loss.item())

plt.plot(losses)
plt.show()
{% endhighlight %}

이외에도 MNIST 손글씨 분류 실습이 존재하지만, 비슷한 내용이므로 넘기도록 하자

### 4. 과적합(overfitting)을 막는 방법들 
학습데이터에 모델이 과적합되는 현상을 모델의 성능을 떨어트리는 주요 이슈다. 
모델이 과적합되면 train acc는 높아지지만, 새로운 데이터를 받아들이는데 어렵기 때문에 모델의 정확도가 매우 낮아진다.

막는 방법으로는 
1. 데이터의 양을 늘리기
모델은 데이터의 양이 적을 경우 쉽게 암기할 수 있으므로 과적합 현상이 발생할 확률이 늘어난다.
적은 데이터셋이라면 조금씩 변형하고 추가하여서 데이터의 양을 늘리기도 하는데 이를 데이터 증강 (data augmentation)기법이라고 한다.
2. 모델의 복잡도 줄이기
인공신경망의 복잡도는 hidden layer의 수나 매개변수의 수 등으로 결정된다. 과적합이 된다면 이 복잡도를 죽이는 것이 중요하다.
3. 가중치 규제(Regularization) 적용하기
복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다. 그리고 간단한 모델이란, 적은수의 파라미터를 가진 모델을 의미한다. 복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제가 있다.
L1 norm, L2 norm 이라고 하는데, L1은 가중치들의 절대값 합계를 loss 함수에 추가하는 것이고 L2는 모든 가중치들의 제곱합을 loss함수에 추가하는 것이다.
loss 함수에 weight_decay를 사용하는 것이 이 방법이다. 
4. dropout(드롭아웃)
드롭아웃은 학습과정에서 신경망의 일부를 사용하지 않는 방법이다.
학습시에만 사용하고 예측시에는 사용하지 않는 것이 일반적이다. 학습 시에 인공신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 막아주고 랜덤으로 선택해주므로 서로 다른 신경망을 앙상블하여 학습하는 효과가 나타난다.

### 5. 기울기 소실과 폭주
앞에서 말한것과 같이 은닉층에 sigmoid 함수를 사용하게 되면 기울기가 소실되는 문제가 발생할 수도 있다. 하지만 기울기가 너무 커져서 발산되버리는 기울이 폭주가 발생할 수도 있다. 이는 Gradient exploding이라고 한다.
이를 방지하기 위한 방법을 설명한다.

1. ReLU와 변형들
앞에서 얘기했으니 넘어가자
2. 가중치 초기화
같은 모델을 학습시키더라도 가중치가 초기에 어떤 값을 가졌냐에 따라서 모델의 학습 성능이 달라진다. 즉 초기화만 적절하게 해줘도 기울기 소실 문제나 폭발을 방지할 수 있다는 말이 된다.
세이비어 초기화(Xavier initialization)은 glorot 초기화라고도 부른다. 이 방법은 균등 분포 혹은 정규분포(unform distribution or normal distribution)으로 초기화 할 때로 나뉘며 이전 층의 뉴런 개수와 다음 층의 뉴런 개수로 식을 세운다.
이 초기화 방식은 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 너무 주목을 못받는 것을 막는다.
이 초기화 방식은 sigmoid 나 Tanh 함수와 같이 S자 형태인 활성화 함수와는 좋은 성능을 보이지만, ReLU와는 좋지 않은 성능을 보여준다. 
ReLU계열 함수와는 He initialization 을 사용하는 것이 일반적이다.
3. 배치 정규화
batch normalization은 표현 그대로 한번에 들어오는 배치 단위로 정규화하는 것을 의미한다. 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행된다. 배치 정규화를 요약하면 입력에 대해서 평균을 0으로 만들고 정규화를 진행한다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행한다. 이를 사용하면 sigmoid나 TanH함수를 사용할 때 기울기 소실 문제가 해결되고 가중치 초기화에 덜 민감해진다. 배치단위로 정규화를 하기 때문에 노이즈를 넣는 효과를 가지고 있어서 과적합을 방지할 수 있다. 
하지만 모델을 복잡하게 하고 시간이 오래걸린다.
4. 층 정규화 (layer normalization)
한 layer에 들어오는 데이터들로 정규화를 진행하여서 저장하는 것이다. 잘 안쓰는듯 


### 6. 마무리 
이번편까지 최대한 빠르고 내가 놓치고 있던 개념들을 다시 공부해봤다. 
다음편부터는 실습위주로 내가 잘 못하는 분야인 CNN RNN 들을 공부해볼 것이다.
이렇게 기초를 다시 공부하면서 예전에 흘려넘겼던 정보들이 눈에 밟히고 모델들을 더 잘 이해하게 되는것 같다.
수학적으로도 이렇게 증명이 되는구나를 알게 되는 것이 매우 좋다.