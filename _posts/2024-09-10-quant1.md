---
layout: post
title:  "Quantization - 1"
excerpt: "Deep Learning, Quantization"


tags:
  - [Blog, jekyll, Github, Python, Quantization]

toc: true
toc_sticky: true
 
date: 2024-09-10
last_modified_at: 2024-09-10
---
{% include toc.html %}

### Quantization    
Quantization (양자화)란, 모델의 실행 속도 향상을 위해 신경망의 weight, activation function의 출력을 더 작은 bit로 표현하는 기술을 의미한다.    
개념적으론 쉽지만 생소한 부분이기도 하고, 실무에서 큰 모델을 사용하면서 불편했기 때문에 공부하고 적용할 수 있는 능력을 만들기 위해서 정리를 시작할 것이다.    
     
기본적으로 tensorflow나 pytorch의 파라미터는 보통 32비트 부동 소수점 연산이다.    
하지만 quantization은 이를 8비트 정수로 표현한다.    
이렇게 하면 장점이 대략    
1. 모델 크기 4배 감소   
2. 메모리 대역폭 2-4배 감소    
3. 빠른 계산으로 인한 시간 감소   
등이 존재한다.    
하지만 int8의 경우 255개의 정보만 표현할 수 있고, 이는 곧 정보의 손실로 간다. 미묘한 차이를 잡아내지 못하므로 모델의 성능이 감소하는 것이다.   
따라서 quantization을 진행할 때는 어느정도 정확도의 유실과 속도를 타협해야한다.    
또 이런 손실의 경우 layer가 많아서 많은 곱이 들어가게 된다면 더욱 심해지는 문제가 있다.    

#### Quantization의 종류 
1. Dynamic Quantization(동적 양자화)    
  가중치는 미리 양자화되어있고, 활성화 함수의 output은 inference할 때 동적으로 양자화가 진행된다.     
2. Static Quantization(정적 양자화)    
  Post Training Quantization이라고 하며, PTQ라고 한다.    
  이 방법은 학습이 종료된 모델을 양자화하는 기법으로 파라미터 size가 큰 모델의 경우 정확도 하락이 적지만, 파라미터가 작은 소형 모델은 정확도 하락이 크다.     
3. Quantization aware training(양자화 인식 교육)     
  training을 진행하면서 양자화가 되었을 때 loss가 어떨지 함께 학습시키는 방법으로, 학습 중 cost가 추가로 든다는 단점이 있다.    


