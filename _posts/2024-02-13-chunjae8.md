---
layout: post
title:  "천재교육 빅데이터 7기 강의 정리 -8"
excerpt: "천재교육 8"

tags:
  - [Blog, jekyll, Github, Python, 천재교육, 빅데이터, 데이터EDA, 데이터 EDA, 데이터분석, 데이터 분석, 상관계수, 회귀계수, 회귀분석, 통계, 클러스터링, 의사결정나무]

toc: true
toc_sticky: true

date: 2024-02-13
last_modified_at: 2024-02-13
---

{% include toc.html %}

### 2월 13일 강의 정리
[깃허브 링크1](https://github.com/gustjr7879/chunjae/blob/main/jae10.ipynb)      
[깃허브 링크2](https://github.com/gustjr7879/chunjae/blob/main/hw4.ipynb)        

오늘은 의사결정나무(Decision Tree)와 K-means 클러스터링에 대해서 배웠다.    
두개는 예전에 많이 접해봤던 내용이라 좀 편안하게 들었지만 코드를 하나하나 상세하게 보고 시각화를 직접해보고 파라미터가 뭐가 있는지, 어떤식으로 구성되는지 알아가는게 유의미했다.    
그러면 이론부터 바로 보도록 하자.     
#### 의사결정나무(Decision Tree)
- 특성(features)를 기준으로 샘플(data)를 분류하는 트리 모델이다.     
- 회귀문제, 분류문제에 모두 적용가능하다는 장점이 있다.     
- 또한 분석결과가 직관적이라서 분석결과를 액션플랜으로 활용하기 좋다.     
- 하지만 tree depth가 깊어지면 과적합(over-fitting)이 되기 쉽다.      
1. 노드와 엣지    
- 의사결정나무에서 샘플을 분류할 때 사용되는 질문이나 그에 따른 값을 나타낸다.     
- 뿌리노드(root) : 의사결정나무의 첫번째 노드이다. 지니 불순도가 가장 작은 feature가 루트노드가 된다.    
- 중간노드(internal node) : 중간노드, 노드의 특성은 지니불순도의 감소가 가장 큰(Information Gain) 지점의 feature로 결정된다.     
- 말단노드(external node) : 결정트리의 가장 밑이다. 최종 결론이며 분류에서는 다수 샘플을 가진 클래스로 결정하고 회귀 모델에서는 평균으로 결정된다.     

즉 의사결정나무는 지니불순도가 작아지도록 업데이트가 된다.     

2. 지니불순도(Gini Impurity)
- 의사결정나무 모델 중 CART 알고리즘의 비용함수이다.    
- 불순도 : 여러 범주(category)가 섞여있는 정도이고 불순도가 낮다는것은 대부분의 값이 같은 값을 가진다는 것을 보여준다.     
- 의사결정나무에서는 불순도를 최소화할 수 있는 지점을 선택한다.    

3. 특성중요도(Feature importance)
- 해당 특성으로 분기했을 때 지니 불순도의 총 감소량이다. 높을수록 의사결정나무의 분류 성능에 영향을 끼치는 feature로 알 수 있다.     
- 단, high-cardinality한 경우 불순도 감소량이 매우 높게 나타나서 중요한 컬럼으로 보여질 수 있다. 이 경우 permutation importance로 보는 것이 좋다.      

의사결정나무는 target을 설정해두고 학습 추론하는 모델이라서 target을 꼭 설정해주자.     

```python
from sklearn.tree import DecisionTreeClassifier
from category_encoders import OrdinalEncoder
encoder = OrdinalEncoder()
from sklearn.impute import SimpleImputer #결측치를 채워준다.
imputer = SimpleImputer()

X_train_encoded = encoder.fit_transform(X_train)
X_train_imputed = imputer.fit_transform(X_train_encoded)
print(X_train_encoded.shape)
model = DecisionTreeClassifier(max_depth=7)
model.fit(X_train_imputed,y_train)

X_val_encoded = encoder.fit_transform(X_test)
X_val_imputed = imputer.fit_transform(X_val_encoded)

print('테스트 정확도',model.score(X_val_imputed,y_test))
```   

위와 같이 할 수 있다. Imputer는 결측치를 채워준다. 또한 Ordinal Encoder를 사용해서 카테고리로 되어있는 데이터를 인코딩해준다(원핫인코딩으로 사용 가능함)     
시각화는 plot_tree를 사용해서 그릴 수 있다.    

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20,10))
plot_tree(model,filled=True,feature_names=list(X_train_encoded.columns),fontsize=10)
plt.show()
```     

또한 특성 중요도 또한 뽑을 수 있다.   

```python
# 특성 중요도 출력
importance = model.feature_importances_
feature_im_df = pd.DataFrame({
    'Feat':X_train_encoded.columns,
    'importance':importance
})
print(feature_im_df.sort_values('importance',ascending=False))
```     

전체 과정과 시각화, 중요도 결과는 깃허브 링크를 들어가서 확인할 수 있다.     

#### K-means Clustering (K-means Clustering)
- 유사한 그룹으로 묶는 알고리즘이고 비지도학습 방법이다.    
- 각 클러스터별 중심점에서 클러스터 내 data point간의 거리의 제곱합을 최소화하면서 업데이트된다.     
- K를 몇개로 설정하냐가 성능에 영향을 끼친다.    
- 최적의 K를 설정하는 방법은 실무에서 필요에 의해 직관적으로 선정하는 방법과 CCC통계량, 스크리플랏(k에 따른 iner- 변화), 엘보우 메소드 등이 있다.    

실행 순서는 다음과 같다.    
- K개의 중심을 랜덤으로 설정한다.    
- 모든 데이터 포인트들은 k개의 중심과 거리를 계산하고, 가장 가까운 중심점으로 클러스터링된다.    
- 클러스터들이 k개 완성되면 그 클러스터 안에 data point들의 평균으로 새로운 중심점을 생성한다.   
- 위의 2개 과정을 반복하면서 수렴한다.    

2가지 feature만 사용하는 코드는 다음과 같다.    

```python
from sklearn.cluster import KMeans

search_k = range(1,10) #1-9에서 k탐색
inertias = [] #inertia는 데이터들이 k-means clustering에 대해서 얼마나 잘 클러스터되어있는지 나타내는 값이다.

for k in search_k:
    model = KMeans(n_clusters=k)
    model.fit(X_scaled)
    inertias.append(model.inertia_)

plt.figure(figsize=(4,4))
plt.plot(search_k,inertias,'-o')
plt.xlabel('K')
plt.ylabel('inertia')
plt.xticks(search_k)
plt.show()
```     

inertia는 얼마나 잘 클러스터 되어있는지(거리) 나타내는 값이다. 반복문을 사용해서 k에 따른 innertia 값을 시각화하는데, 이 방법을 스크리플랏이라고 한다.     

#### 마무리

이 과정을 거쳐서 머신러닝 기법 중 일부인 의사결정나무와 k-means 클러스터링에 대해서 알아보고 직접 실행해보았다.    
이 방법들을 통해서 막연할 때, 그리고 자세하게 알고싶을 때 데이터 분석을 실행할 수 있는 방법을 얻었다.    
내일은 졸업식이라서 오지는 못하지만, 시간날 때 직접 분석해보고 보고서도 한번 작성해볼 예정이다.