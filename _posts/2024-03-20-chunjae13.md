---
layout: post
title:  "천재교육 빅데이터 7기 강의 정리 -13"
excerpt: "천재교육 13"

tags:
  - [Blog, jekyll, Github, Python, 천재교육, 빅데이터, 데이터EDA, 데이터 EDA, 데이터분석, 데이터 분석, 이상치, 결측치, 모델링, 머신러닝, 기계학습]

toc: true
toc_sticky: true

date: 2024-03-20
last_modified_at: 2024-03-20
---

{% include toc.html %}

### 3월 20일 강의 정리
[깃허브 링크1](https://github.com/gustjr7879/chunjae/blob/main/jae18.ipynb)      

### 기계학습
기계학습.. 석사과정에 많이 찾아보고 나름 많이 알고 있지만 빠른 연구를 위해서 관련 없거나 사용하지 않는다 싶으면 개념만 익히고 넘어갔던 부분이 많다.     
이번 기회로 나름 많이 알아가고자 한다.

#### Feature Scaling
데이터를 전처리해서 모델이 학습을 잘할 수 있게 해준다. feature의 범위를 제한하거나, 분포를 그대로 따갈 수 있게 한다.     
너무 값이 큰 feature data는 모델의 학습과정 중에 민감하게 학습하게 한다. feature scaling을 통해서 이를 방지할 수 있다.      

#### 표준화
Feature scaling의 방법중에 하나인 표준화는 데이터를 변형시킬 때 정규분포를 따라가게 만든다. 하지만 범위 제한이 없기 때문에 데이터들이 다양한 범위에 존재할 수 있다.             

```python
sklearn.preprocessing.StandardScaler
```
        

코드로 불러오고 사용할 수 있다.     
예전에 학습 중에도 적어놨지만 정규화 및 표준화를 진행할 땐 꼭 ! train data만 가지고 해야한다. test data가 들어가는 순간 데이터 유출이 될 수 있기 때문이다.     
하지만 이번 실습에서는 신경쓰지 않고 작업했다.       
표준화를 통한 데이터의 변화는 깃허브 링크에 들어가서 확인할 수 있다.     

#### 정규화
정규화의 경우 데이터를 일정한 범위 내로 만들어줘서 모델에서 지나친 영향을 줄여준다.     

```python
sklearn.preprocessing.MinMaxScaler
```     

를 통해 사용할 수 있다. 0과 1사이로 표현된다.     


#### 의사결정나무(Decision Tree)
상당히 많이 나오는 친구다 ^^ ..     
그만큼 성능이 보장되어있고 가장 유명하다고 볼 수 있다.      

##### 장점
사람이 이해할 수 있고, 모델을 시각화할 수 있어서 white box 알고리즘으로 볼 수 있다.      
데이터 전처리를 거의 요구하지 않아서 간편하게 사용할 수 있다.     
모델의 복잡도는 데이터 복잡도에 비례하기 때문에 사용되는 자원을 쉽게 계산할 수 있다.     
숫자와 범주형 데이터를 가리지 않고 학습시킬 수 있기 때문에 급하면 바로 사용해보자..!       

##### 단점
과적합(overfitting)에 취약하다.     
입력데이터의 살짝의 변화만 있어도 트리가 많이 변화할 수 있다.    
입력값의 범위 바깥에서 나타나는 데이터를 예측하기에 적합하지 않다 (과적합과 유사함)     
최적의 Decision tree를 찾는 것은 불가능에 가깝다.      

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
```     
를 통해서 사용할 수 있고 시각화할 수 있다.      
사용되는 모습과 시각화 결과를 보고 싶다면 깃허브 링크를 눌러보자 .. !       


#### 특성 중요도 (feature importance)
머신러닝이 종료되고 이상한 기준으로 분류하는 경우나 맘에 들지 않을 때 feature importance를 통해서 모델의 성능을 향상시킬 수 있다.      
이를 통해 불필요한 feature를 제거할 수 있고 데이터 전처리 과정에서 미흡하거나 놓친 부분을 체크할 수 있다.     
하지만 너무 이것만 보고 다 날려버리면 오히려 모델의 성능이 떨어질 수 있다.      
```python
model.feature_importances_
```      

를 통해서 구할 수 있다.      


#### 교차검증 (Cross Validation)
교차검증이란, 학습중인 모델의 성능을 평가하는 방법으로 고정된 검증 데이터셋을 보통 사용하는데 이 경우에 validation set에 너무 과도하게 학습될 수 있어서 이를 방지하고자 만들어진 기법이다.    
학습데이터셋을 K개로 나누고 하나씩 검증 데이터셋으로 변경해가면서 학습을 하고 검증을 한다. 최종 평가는 K번 평가 결과의 평균값을 사용한다.     

```python 
sklearn.model_selection import KFold
```     
를 통해서 만들 수 있다.      


#### StratifiedKFold
만약 셔플된 데이터에서 class가 너무 unbalance하게 되어있는 경우를 방지할 수 있다.     
class의 비율을 따져가면서 kfold를 진행한다.      

#### Hyperparameter Search
기계학습과 딥러닝에는 하이퍼파라미터라는 것이 존재한다. 이는 모델의 학습과정에 사용되지 않아서 직접 설정해줘야한다.     
학습에 사용되지 않기 때문에 사람이 직접 설정해줘야되고 이에따라서 모델의 성능이 크게 변할 수 있기 때문에 최적의 파라미터를 구해야한다.     
GridSearch CV, RandomSearch CV 등의 방법이 있다.     


#### GridSearch Cross Validation
GridSearchCV는 가능한 모든 하이퍼 파라미터의 조합 중 최적의 조합을 찾는 과정이다.      
모든 경우의 수를 따지기 때문에 컴퓨팅 파워가 많이 들고 시간이 녹는다.     

```python
from sklearn.model_selection import GridSearchCV
```     

를 통해서 알아볼 수 있고 코드를 보면      

```python
parameters = {'max_depth':range(1,10),'min_samples_split':range(2,5),'min_samples_leaf':range(1,5)} #이와 같이 하이퍼파라미터의 범위를 설정해준다. key-value로
grid = GridSearchCV(d_tree_,param_grid=parameters,cv=3,refit=True,return_train_score=True) #3fold cv로 최적의 결과를 탐색한다. 이는 cv_results_로 저장되고 조회해서 조합을 찾아볼 수 있다.
```     

처럼 사용할 수 있다. 일정 범위 내에서 최적의 파라미터를 찾고 refit을 통해서 그 파라미터로 갱신해준다.     

```python
print('Best parameters', grid.best_params_)
print('Best Score', grid.best_score_)
```
로 최적의 파라미터와 정확도를 찾아낼 수 있다.     


#### RandomSearch Cross Validation      
GridSearch의 최고 단점인 오랜 시간이 걸린다는 것을 방지하기 위해서 랜덤하게 체크한다.     
최적의 결과는 아니지만 나름 적은 자원으로 만족스러운 결과를 얻을 수 있다.     

```python
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=d_tree_,param_distributions=parameters,n_iter=10,cv=3,random_state=42,n_jobs=1)
random_search.fit(X_train,y_train)
```     

와 같이 사용할 수 있고, n_iter는 몇번 탐색할 것이지 설정해줄 수 있다.      
결과는 GridSearch와 비슷하다.     
하지만 빠르게 결과를 내고 싶을 때 사용하는 방법이고 보통 최고의 성능을 내서 저장해서 사용하기 때문에 시간이 오래걸려도 GridSearch를 사용한다. 