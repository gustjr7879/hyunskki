---
layout: post
title:  "Pytorch를 공부해보자"
excerpt: "파이토치 문법 및 기본기 정리"


tags:
  - [Blog, jekyll, Github, Pytorch]

toc: true
toc_sticky: true
 
date: 2022-12-01
last_modified_at: 2022-12-01
---
{% include toc.html %}

### Pytorch란?
#### 파이토치
파이토치는 딥러닝에 자주 사용되는 프레임워크이고 상당히 많이 사용된다.
기존에 사용하던 데이터 형식에서 변환해줘야 사용할 수 있다는 점이 있는다 이 부분에서 많이 혼동스러워서 정리해보려고한다.
#### Pytorch 패키지 구성 요소
* torch
		* main namespace로 tensor등의 다양한 수학 합수가 패키지에 포함되어 있음
		* numpy와 같은 구조를 가지고 있어서 비슷한 문법 구조를 사용할 수 있음
* torch.autograd
		* 자동 미분을 위한 함수가 포함되어 있음
		* 자동 미분의 on, off를 제어하는 enable_grad 또는 no_grad나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 Function등이 포함됨
* torch.nn
		* 신경망을 구축하기 위한 다양한 데이터 구조나 레이어가 정의되어 있음
		* CNN, LSTM, Activation function(ReLu 등), loss 등이 정의되어 있음
* torch.optim
		* SGD 등의 파라미터 최적화 알고리즘이 구현되어 있음
* torch.utils.data
		* Gradient Descent 계열의 반복 연산을 할 때 사용하는 minibatch용 유틸리티 함수가 포함되어있음
* torch.onnx
		* ONNX(Open Neural Network eXchange) 포맷으로 모델을 export 할 때 사용함
		* ONNX는 서로 다른 딥러닝 프레임워크 간에 모델을 공유할 때 사용하는 새로운 포맷임 (아직 한번도 안써봤음)
 
#### Pytorch 기초 사용법



{% highlight css %}
import torch

nums = torch.arange(9)
# : tensor([0,1,2,3,4,5,6,8])
{% endhighlight %}

* 위와 같이 pytorch는 tensor라는 것을 사용하여 생성할 수 있음

{% highlight css %}
nums.shape
# torch.Size([9])

type(nums)
# torch.Tensor
{% endhighlight %}

* 여기까지 보면 numpy와 비슷한 문법 구조를 보여주고 있다.

{% highlight css %}
nums.numpy()
# tensor 를 numpy로 타입 변환 (가끔 텐서 문법 생각 안날 때 넘파이로 바꿔서 변환하고 다시 텐서로 변환했음)
# array([0,1,2,3,4,5,6,7,8],dtype = int64)

nums.reshape(3,3)
# tensor([ [0,1,2],
#						[3,4,5],
#						[6,7,8] ])

nums = torch.arange(9).reshape(3,3)
nums
# tensor([[0,1,2],
#          [3,4,5],
#          [6,7,8]])
nums + nums
# tensor([[ 0,  2,  4],
#         [ 6,  8, 10],
#         [12, 14, 16]])
{% endhighlight %}

* 다음으로는 Tensor를 직접 생성하고 조작하는 방법을 공부함

#### 텐서의 생성과 변환
* 텐서는 pytorch의 가장 기본이 되는 데이터 구조와 기능을 제공하는 다차원 배열을 처리하기 위한 데이터 구조
* API 형태는 numpy의 ndarray와 비슷하며 GPU를 사용하는 계산도 지원함(이 때문에 딥러닝에서 많이 씀)
* 텐서는 각 데이터 형태별로 정의되어 있음
		* torch.FloatTensor : 32 bit float point
		* torch.LongTensor : 64 bit signed integer
* GPU 상에서 계산할 때는 torch.cuda.FloatTensor를 사용함. 일반적으로 Tensor는 FloatTensor라고 생각하면 됨
* 어떤 형태의 텐서이던지 torch.tensor라는 함수로 작성 가능

{% highlight css %}
import torch
import numpy as np

# 2차원 형태릐 list를 이용하여 텐서를 생성할 수 있습니다.
torch.tensor([[1,2],[3,4.]])
# : tensor([[1., 2.],
#         [3., 4.]])
        
# device를 지정하면 GPU에 텐서를 만들 수 있습니다.
torch.tensor([[1,2],[3,4.]], device="cuda:0")
# : tensor([[1., 2.],
#         [3., 4.]], device='cuda:0')
        
# dtype을 이용하여 텐서의 데이터 형태를 지정할 수도 있습니다.
torch.tensor([[1,2],[3,4.]], dtype=torch.float64)
# : tensor([[1., 2.],
#         [3., 4.]], dtype=torch.float64)
        
# arange를 이용한 1차원 텐서
torch.arange(0, 10)
# : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

# 모든 값이 0인 3 x 5의 텐서를 작성하여 to 메소드로 GPU에 전송
torch.zeros(3, 5).to("cuda:0")
# :tensor([[0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.]], device='cuda:0')
        
# normal distribution으로 3 x 5 텐서를 작성
torch.randn(3, 5)
# : tensor([[-0.4615, -0.4247,  0.1998, -0.5937, -0.4767],
#         [ 0.7864,  0.3831, -0.7198, -0.0181, -1.1796],
#         [-0.4504, -1.3181,  0.2657,  0.6829, -1.1690]])
        
# 텐서의 shape은 size 메서드로 확인
t = torch.randn(3, 5)
t.size()
# : torch.Size([3, 5])
{% endhighlight %}

* 텐서는 Numpy의 ndarray로 쉽게 변환할 수 있다.
* 단, GPU상의 텐서는 그대로 변환할 수 없으며, CPU로 이동 후에 변환해야함

{% highlight css %}
# numpy를 사용하여 ndarray로 변환
t = torch.tensor([[1,2],[3,4.]])
x = t.numpy()

# GPU 상의 텐서는 to 메서드로 CPU의 텐서로 변환 후 ndarray로 변환해야 합니다.
t = torch.tensor([[1,2],[3,4.]], device="cuda:0")
x = t.to("cpu").numpy()
{% endhighlight %}

* torch.linspace(시작, 끝, step)
	* 시작과 끝을 포함하고 step의 갯수만큼 원소를 가진 등차 수열을 만듬
	* 예를 들어 torch.linspace(0, 10, 5) 라고 하면
	* tensor([0.0, 2.5, 5.0, 7.5, 10.0]) 의 값을 가짐
* torch 에서 이런 값을 만들면 torch 내부적으로도 사용할 수 있지만 numpy로도 변환하여 사용가능

{% highlight css %}
x = torch.linspace(0, 10, 5)
y = torch.exp(x)

plt.plot(x.numpy(), y.numpy())
{% endhighlight %}

#### 텐서의 인덱스 조작

* 텐서의 인덱스를 조작하는 방법은 여러가지가 있다.
* 텐서는 numpy 의 ndarray와 같이 조작하는 것이 가능함. 배열처럼 인덱스를 바로 지정하고 슬라이스,
* 마스크 배열을 사용할 수 있음

{% highlight css %}
t = torch.tensor([
    [1,2,3],[4,5,6.]
])

# 인덱스 접근
t[0, 2]
: tensor(3.)

# 슬라이스로 접근
t[:, :2]
: tensor([[1., 2.],
        [4., 5.]])
        
# 마스크 배열을 이용하여 True 값만 추출
t[t > 3]
: tensor([4., 5., 6.])

# 슬라이스를 이용하여 일괄 대입
t[:, 1] = 10

# 마스크 배열을 사용하여 일괄 대입
t[t > 5] = 20
{% endhighlight %}

#### 텐서 연산

* 텐서는 numpy의 ndarray와 같이 다양한 수학 연산이 가능하고 GPU를 사용하여 빠른 연산이 가능함
* 텐서에서 사칙연산은 같은 타입의 텐서 간 또는 텐서와 파이썬의 스칼라 값 사이에서만 가능함
		* 텐서 간이라도 타입이 다르면 연산 안됨
* 스칼라 값을 연산할 때 기본적으로 broadcasting이 지원됨

{% highlight css %}
# 길이 3인 벡터
v = torch.tensor([1,2,3.])
w = torch.tensor([0, 10, 20])

# 2 x 3의 행렬
m = torch.tensor([
    [0, 1, 2], [100, 200, 300.]
])

# 벡터와 스칼라의 덧셈
v2 = v + 10

# 제곱
v2 = v ** 2

# 동일 길이의 벡터간 덧셈 연산
z =  v - w

# 여러 가지 조합
u = 2 * v - w / 10 + 6.0

# 행렬과 스칼라 곱
m2 = m * 2.0

# (2, 3)인 행렬과 (3,)인 벡터간의 덧셈이므로 브로드캐스팅 발생
m + v

:tensor([[  1.,   3.,   5.],
        [101., 202., 303.]])
        
# 행렬 간 처리
m + m

:tensor([[  0.,   2.,   4.],
        [200., 400., 600.]])
{% endhighlight %}