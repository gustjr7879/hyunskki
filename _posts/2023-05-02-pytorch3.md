---
layout: post
title:  "Deep learning-3"
excerpt: "Deep Learning-3"

tags:
  - [Blog, jekyll, Github, Python]

toc: true
toc_sticky: true
 
date: 2023-05-02
last_modified_at: 2023-05-02
---
{% include toc.html %}


### 1. 로지스틱 회귀(Logistic Regression)
일상속에서 풀고자 하는 문제중에서 두개의 선택지 중 하나를 고르는 문제들이 있다. 이렇게 둘 중 하나를 선택하는 것을 이진 분류 문제라고 하는데, 이진분류를 풀기위한 대표적인 방법으로는 로지스틱 회귀가 있다.

로지스틱 회귀 알고리즘의 이름은 회귀이지만, 실제로는 분류 문제에서 사용 가능하다.

만약 45,50,55,60,65,70 이 x데이터로 들어가고 y 데이터로 0,0,0,1,1,1이 출력된다고 할 때 이를 표시하려면 직선으로 분류하는 것이 아니라, S자 형태로 표현할 수 있는 함수가 필요하다.
즉, H(x) = Wx + b 의 형태가 아닌 H(x) = f(Wx + b)의 형태가 필요하다는 말이 된다.
S자 형태로 널리 알려진 함수는 시그모이드함수가 있다.

{% highlight css %} 
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1/(1+np.exp(-x))

#W값에 따른 경사도가 변하는 것을 체크할 수 있다.

x = np.arange(-5.0,5.0,0.1)
y1 = sigmoid(0.5*x)
y2 = sigmoid(x)
y3 = sigmoid(2*x)

plt.plot(x,y1,'r')
plt.plot(x,y2,'g')
plt.plot(x,y3,'b')

plt.plot([0,0],[1.0,0.0],':')
plt.title('sigmoid function')
plt.show()


#b 값의 변화에 따른 좌우 이동
x = np.arange(-5.0,5.0,0.1)
y1 = sigmoid(x+0.5)
y2 = sigmoid(x+1)
y3 = sigmoid(x+1.5)

plt.plot(x,y1,'r')
plt.plot(x,y2,'g')
plt.plot(x,y3,'b')

plt.plot([0,0],[1.0,0.0],':')
plt.title('sigmoid function')
plt.show()
{% endhighlight %}
위의 코드를 가지고 W값에 따른 기울기의 변화와 b 값에 따라서 좌우가 이동되는 것을 확인할 수 있다.

Sigmoid 함수는 입력값이 한없이 커지면 1에 수렴하고, 작아지면 0에 수렴하는 것을 볼 수 있다. 즉 0과 1사이의 값을 얻는데 이를 활용하여서 분류 문제에 사용할 수 있다.

#### sigmoid와 loss function
이제 시그모이드를 사용하여서 loss를 계산하려는데 문제가 발생한다.
이전 실습에서 사용했던 MSE 오차를 사용하면 선형회귀때와 달리 로컬미니멈이 생기는 것을 볼 수 있다. 즉 최적의 해를 찾아가지 않고 멈춰버려서 모델의 성능이 고정되어버리는 것을 볼 수 있다.

sigmoid는 0.5를 기준으로 0과 1로 구분되는 결과를 보여주는데, 이 특성상 loss를 계산할 때 0.5를 기준으로 대칭인 로그함수가 나타난다.
이는 
https://wikidocs.net/57805 에 들어가서 확인하도록 하자.

파이토치로 로지스틱 회귀를 실습해보자

{% highlight css %} 
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


torch.manual_seed(1)
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

# 모델 초기화
W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=1)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # Cost 계산
    hypothesis = torch.sigmoid(x_train.matmul(W) + b)
    cost = -(y_train * torch.log(hypothesis) + 
             (1 - y_train) * torch.log(1 - hypothesis)).mean()

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, nb_epochs, cost.item()
        ))

prediction = hypothesis >= torch.FloatTensor([0.5])

print(prediction)


x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

model = nn.Sequential(
   nn.Linear(2, 1), # input_dim = 2, output_dim = 1
   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다
)

model(x_train)

# optimizer 설정
optimizer = optim.SGD(model.parameters(), lr=1)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    hypothesis = model(x_train)

    # cost 계산
    cost = F.binary_cross_entropy(hypothesis, y_train)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 20번마다 로그 출력
    if epoch % 10 == 0:
        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주
        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주
        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산
        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력
            epoch, nb_epochs, cost.item(), accuracy * 100,
        ))
{% endhighlight %}

아래와 같이 나오는데, 두번째 실습된 내용은 nn.Module로 구현하는 로지스틱 회귀방식이다. nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 한다. 그래서 nn.Linear(2,1)과 nn.Sigmoid()함수를 사용하여서 한번에 진행할 수 있게 한다.

### 2. 인공신경망으로 표현되는 로지스틱 회귀
사실 로지스틱 회귀는 인공신경망으로 간주할 수 있다.
뉴런 x1, x2, 1이 있다고 할 때 
W1, W2와 x1 x2가 곱해지고 1이 더해지는 로지스틱 회귀로 표현할 수 있기때문이다. 
모델을 클래스로 구현하는것은 이전에 했던 실습에서 linear층 뒤에 sigmoid층을 추가해줌으로서 구할 수 있다.

### 3. 소프트맥스 회귀(Softmax Regression)
소프트맥스 회귀를 다루기 이전에 원-핫인코딩에 대해서 읽어보는 시간을 가지면 좋다.
나는 바로 소프트맥스 회귀를 다루도록 하겠다.

이진분류가 두개의 답 중 하나를 고르는 것이였다면, 세 개 이상의 답 중 하나를 고르는 것은 다중 클래스 분류라고 한다. 꽃의 품종 분류 문제와 같이 자주 사용되는 태스크이고, 내가 다루는 분야에서도 전부 다 다중 클래스 분류 문제를 다루고 있다.

로지스틱 회귀에서 시그모이드 함수는 0-1사이의 값을 반환해서 이진분류를 하면 0.5 이상의 값을 가지면 T로 표현할 수 있었다.

소프트맥스 회귀는 확률의 총 합이 1이 되는 아이디어를 다중 클래스 분류문제에서 사용한다. 즉 로지스틱회귀를(sigmoid)하면 여러개의 input에서 1개의 값만 나오지만, softmax 회귀를 진행하면 여러개의 input에서 class 갯수에 맞는 값이 나와서 해당 클래스가 몇퍼센트 확률로 나올 수 있는지를 알 수 있게된다.

말로 풀면 좀 어려울 수 있으니,https://wikidocs.net/59427
에 들어가서 확인해보도록 하자.

이렇게 소프트맥스 회귀, 로스함수 등을 알아보게 되었다.

이제 소프트맥스 회귀를 사용하여서 MNIST 데이터 분류를 진행해보자

practice7.py 에서 실습한 내용을 찾아볼 수 있고 여기에서 기억정도 해놔야할 부분은 data_loader를 어떻게 사용하는지, 어떻게 view를 이용하여서 input의 모양을 바꾸는지 등이 있다. 
{% highlight css %} 
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import random


USE_CUDA = torch.cuda.is_available()
device = torch.device('cuda' if USE_CUDA else 'cpu')
print('device on', device)

random.seed(777)
torch.manual_seed(777)

training_epochs = 15
batch_size = 128

mnist_train = dsets.MNIST(root='MNIST',train=True,transform=transforms.ToTensor(),download=True)
mnist_test = dsets.MNIST(root='MNIST',train=False,transform=transforms.ToTensor(),download=True)

data_loader = DataLoader(mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)
#drop last 는 배치사이즈로 다 쪼개고 나서 나머지를 버릴것인지 말것인지를 판단해주는데, 보통 마지막에 남는애들은 경사하강법을 진행할 때 과대평가되기 때문이다.

linear = nn.Linear(784,10,bias=True).to(device)
loss_func = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(linear.parameters(),lr = 1e-1)

for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = len(data_loader)

    for X,Y in data_loader:
        X = X.view(-1,28*28).to(device)
        Y = Y.to(device)
        optimizer.zero_grad()
        pred = linear(X)
        loss = loss_func(pred,Y)
        loss.backward()
        optimizer.step()
        avg_cost += loss / total_batch
    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))
# 테스트 데이터를 사용하여 모델을 테스트한다.
with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.
    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = linear(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다
    r = random.randint(0, len(mnist_test) - 1)
    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)

    print('Label: ', Y_single_data.item())
    single_prediction = linear(X_single_data)
    print('Prediction: ', torch.argmax(single_prediction, 1).item())

    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')
    plt.show()
{% endhighlight %}
코드는 이와 같다.